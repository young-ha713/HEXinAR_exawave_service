{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "weak_signal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOX0Fgef+YFzWqy0nMCdfwt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8xCwKZshOa9",
        "outputId": "bac5da42-f32d-4232-f716-903b9b1bb913"
      },
      "source": [
        "! pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Oy9l_86imbb"
      },
      "source": [
        "import pandas as pd\n",
        "d2013 = pd.read_excel('d2013.xlsx')\n",
        "d2014 = pd.read_excel('d2014.xlsx')\n",
        "d2015 = pd.read_excel('d2015.xlsx')\n",
        "d2016 = pd.read_excel('d2016.xlsx')\n",
        "d2017 = pd.read_excel('d2017.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVjrj63EimOk"
      },
      "source": [
        "d2013.to_csv('b2013.csv', index=False, encoding='utf-8-sig')\n",
        "d2014.to_csv('b2014.csv', index=False, encoding='utf-8-sig')\n",
        "d2015.to_csv('b2015.csv', index=False, encoding='utf-8-sig')\n",
        "d2016.to_csv('b2016.csv', index=False, encoding='utf-8-sig')\n",
        "d2017.to_csv('b2017.csv', index=False, encoding='utf-8-sig')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7motiTLgtDS",
        "outputId": "5bdec8b9-67ce-472e-a2d8-49c320de72c8"
      },
      "source": [
        "import csv\n",
        "import glob\n",
        "import re\n",
        "from collections import Counter\n",
        "from konlpy.tag import Twitter; t = Twitter()\n",
        "\n",
        "def condition(nouns):\n",
        "    nouns3 = []\n",
        "    for noun in nouns:\n",
        "        if len(noun) > 1:\n",
        "           nouns3.append(noun)\n",
        "    return nouns3\n",
        "\n",
        "#filelist = glob.glob('*.csv')\n",
        "\n",
        "filelist = [\"b2013.csv\",\"b2014.csv\",\"b2015.csv\",\"b2016.csv\",\"b2017.csv\"]\n",
        "\n",
        "for file in filelist:\n",
        "    sname = file.split(\".\")[0]\n",
        "    documents = open(\"%s\" % file, \"r\").readlines()\n",
        "\n",
        "#documents1 = documents[1:300]\n",
        "\n",
        "    tflist = []\n",
        "    dflist = []\n",
        "\n",
        "    for document in documents:\n",
        "        doc = document.replace('\\r','').replace('\\n','')\n",
        "        doc1 = doc.split(\"\\t\")[-1]\n",
        "        doc2 = re.sub('[^ ㄱ-ㅣ가-힣]+', '', doc1)\n",
        "    #   documentslist.append(hannanum.nouns(doc1)))\n",
        "        #nouns3 = t.morphs(doc2)\n",
        "        morph = condition(t.morphs(doc2))\n",
        "        tflist.append(morph)\n",
        "        dflist.append(list(set(morph)))\n",
        "\n",
        "    docu = len(dflist)\n",
        "\n",
        "    #리스트 통합\n",
        "    tflist1 = sum(tflist, [])\n",
        "    dflist2 = sum(dflist, [])\n",
        "\n",
        "    stopwords =[\"으로\",\"당해\",\"극빈\",\"학사\",\"잇따라\",\"에서\",\"습니다\",\"있다\",\"이다\",\"기자\",\"까지\",\"하는\",\"다는\",\"다고\",\"면서\",\"있는\",\"에는\",\"하고\",\"에서는\",\"밝혔\",\"부터\",\"되는\",\"으며\",\"나리\",\"이번\",\"말했\",\"위해\",\"발행했\",\"때문\",\"지난\",\"다는\",\"모두\",\"경우\",\"입니\",\"지만\",\"하면\",\"일부\",\"따라\",\"통해\",\"이라고\",\"함께\",\"특히\",\"다며\",\"오늘\",\"이나\",\"대해\",\"에서도\",\"있던\",\"정확한\",\"있어\",\"억원\",\"거나\",\"많은\",\"한편\",\"곳곳\",\"관련\",\"하기\"\"같은\",\"입었\",\"가운데\",\"발생한\",\"사실\",\"없는\",\"있으\",\"겪었\",\"있었\",\"연합뉴스\",\"배포\",\"무단\",\"제보\",\"금지\",\"기사\",\"뉴스\",\"사진\",\"오후\",\"문의\",\"발생했\",\"최신\",\"만에\",\"하기\",\"저작권\",\"없었\",\"설명했\",\"위한\",\"채팅\",\"다른\",\"네이버\",\"구독\",\"영상\",\"조사하고\",\"라고\",\"니다\",\"페이스북\",\"여러분\",\"발생해\",\"으나\",\"기다립\",\"필요한\",\"않는\",\"아요\",\"클릭\",\"트렌드\",\"기다립니\",\"보기\",\"제공\",\"코리아\",\"일대\",\"소중한\",\"아내\",\"생생\",\"또는\",\"공감\",\"여기\",\"에게\",\"일어났\",\"나타났\",\"뉴시스\",\"따르\",\"현재\",\"대한\",\"이어\",\"중이\",\"지난해\",\"않았\",\"후회\",\"스탠드\",\"복\",\"카카오\",\"헤럴드경제\",\"독자\",\"언론\",\"글로벌\",\"단\",\"판단\",\"스포츠\",\"포토\",\"보내\",\"동아닷컴\",\"복제\",\"전재\"]\n",
        "\n",
        "    # stopwords 를 제거한 토큰들\n",
        "    tf = [w1 for w1 in tflist1 if not w1 in stopwords]\n",
        "    df = [w2 for w2 in dflist2 if not w2 in stopwords]\n",
        "\n",
        "    # 단어 빈도 카운트\n",
        "    tfcount = Counter(tf)\n",
        "    dfcount = Counter(df)\n",
        "\n",
        "    #df = DataFrame(count)\n",
        "\n",
        "    # 딕셔너리를 데이터 프레임 형식으로 저장\n",
        "    word = csv.writer(open(\"%s\" % sname + \"_tf.csv\", \"w\"))\n",
        "    for key, val in tfcount.items():\n",
        "        word.writerow([key, val])\n",
        "\n",
        "    d = csv.writer(open(\"%s\" % sname + \"_df.csv\", \"w\"))\n",
        "    for key, val in dfcount.items():\n",
        "        d.writerow([key, val])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6SFVXJ2xUsu",
        "outputId": "8c2fefec-1af3-49ad-a89d-22107619bc85"
      },
      "source": [
        "import csv\n",
        "import glob\n",
        "import re\n",
        "from collections import Counter\n",
        "from konlpy.tag import Twitter; t = Twitter()\n",
        "\n",
        "def condition(nouns):\n",
        "    nouns3 = []\n",
        "    for noun in nouns:\n",
        "        if len(noun) > 1:\n",
        "           nouns3.append(noun)\n",
        "    return nouns3\n",
        "\n",
        "# filelist = glob.glob('*.csv')\n",
        "\n",
        "filelist = [\"b2013.csv\",\"b2014.csv\",\"b2015.csv\",\"b2016.csv\",\"b2017.csv\"]\n",
        "\n",
        "for file in filelist:\n",
        "    sname = file.split(\".\")[0]\n",
        "    documents = open(\"%s\" % file, \"r\").readlines()\n",
        "\n",
        "# documents1 = documents[1:300]\n",
        "\n",
        "    tflist = []\n",
        "    dflist = []\n",
        "\n",
        "    for document in documents:\n",
        "        doc = document.replace('\\r','').replace('\\n','')\n",
        "        doc1 = doc.split(\"\\t\")[-1]\n",
        "        doc2 = re.sub('[^ ㄱ-ㅣ가-힣]+', '', doc1)\n",
        "    #   documentslist.append(hannanum.nouns(doc1)))\n",
        "        #nouns3 = t.morphs(doc2)\n",
        "        morph = condition(t.morphs(doc2))\n",
        "        tflist.append(morph)\n",
        "        dflist.append(list(set(morph)))\n",
        "\n",
        "    docu = len(dflist)\n",
        "\n",
        "    #리스트 통합\n",
        "    tflist1 = sum(tflist, [])\n",
        "    dflist2 = sum(dflist, [])\n",
        "\n",
        "    stopwords =[\"으로\",\"당해\",\"극빈\",\"학사\",\"잇따라\",\"에서\",\"습니다\",\"있다\",\"이다\",\"기자\",\"까지\",\"하는\",\"다는\",\"다고\",\"면서\",\"있는\",\"에는\",\"하고\",\"에서는\",\"밝혔\",\"부터\",\"되는\",\"으며\",\"나리\",\"이번\",\"말했\",\"위해\",\"발행했\",\"때문\",\"지난\",\"다는\",\"모두\",\"경우\",\"입니\",\"지만\",\"하면\",\"일부\",\"따라\",\"통해\",\"이라고\",\"함께\",\"특히\",\"다며\",\"오늘\",\"이나\",\"대해\",\"에서도\",\"있던\",\"정확한\",\"있어\",\"억원\",\"거나\",\"많은\",\"한편\",\"곳곳\",\"관련\",\"하기\"\"같은\",\"입었\",\"가운데\",\"발생한\",\"사실\",\"없는\",\"있으\",\"겪었\",\"있었\",\"연합뉴스\",\"배포\",\"무단\",\"제보\",\"금지\",\"기사\",\"뉴스\",\"사진\",\"오후\",\"문의\",\"발생했\",\"최신\",\"만에\",\"하기\",\"저작권\",\"없었\",\"설명했\",\"위한\",\"채팅\",\"다른\",\"네이버\",\"구독\",\"영상\",\"조사하고\",\"라고\",\"니다\",\"페이스북\",\"여러분\",\"발생해\",\"으나\",\"기다립\",\"필요한\",\"않는\",\"아요\",\"클릭\",\"트렌드\",\"기다립니\",\"보기\",\"제공\",\"코리아\",\"일대\",\"소중한\",\"아내\",\"생생\",\"또는\",\"공감\",\"여기\",\"에게\",\"일어났\",\"나타났\",\"뉴시스\",\"따르\",\"현재\",\"대한\",\"이어\",\"중이\",\"지난해\",\"않았\",\"후회\",\"스탠드\",\"복\",\"카카오\",\"헤럴드경제\",\"독자\",\"언론\",\"글로벌\",\"단\",\"판단\",\"스포츠\",\"포토\",\"보내\",\"동아닷컴\",\"복제\",\"전재\"]\n",
        "\n",
        "    # stopwords 를 제거한 토큰들\n",
        "    # tf: 1개 뉴스에서 특정 단어가 나온 횟수를 카운팅(단어빈도) -> 단어의 이슈성 파악\n",
        "    # df: 1개 뉴스에서 여러 단어가 나오더라도 하나만 카운팅(문서빈도) -> 단어의 확산성 파악\n",
        "    tf = [w1 for w1 in tflist1 if not w1 in stopwords]\n",
        "    df = [w2 for w2 in dflist2 if not w2 in stopwords]\n",
        "\n",
        "    # 단어 빈도 카운트\n",
        "    tfcount = Counter(tf)\n",
        "    dfcount = Counter(df)\n",
        "\n",
        "    #df = DataFrame(count)\n",
        "\n",
        "    # 딕셔너리를 데이터 프레임 형식으로 저장\n",
        "    word = csv.writer(open(\"%s\" % sname + \"_tf.csv\", \"w\"))\n",
        "    for key, val in tfcount.items():\n",
        "        word.writerow([key, val])\n",
        "\n",
        "    d = csv.writer(open(\"%s\" % sname + \"_df.csv\", \"w\"))\n",
        "    for key, val in dfcount.items():\n",
        "        d.writerow([key, val])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8hGQMfthQQe"
      },
      "source": [
        "from pandas import Series, DataFrame\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import glob\n",
        "import math\n",
        "\n",
        "# 연도별 weight값 지정\n",
        "weight2013 = 0.8\n",
        "weight2014 = 0.85\n",
        "weight2015 = 0.9\n",
        "weight2016 = 0.95\n",
        "weight2017 = 1\n",
        "\n",
        "# 연도별 단어빈도 데이터 불러오기\n",
        "tf2013 = pd.read_csv(\"b2013_tf.csv\", sep=',', header= 0, names=[\"term\",\"tf_b2013\"])\n",
        "tf2014 = pd.read_csv(\"b2014_tf.csv\", sep=',', header= 0, names=[\"term\",\"tf_b2014\"])\n",
        "tf2015 = pd.read_csv(\"b2015_tf.csv\", sep=',', header= 0, names=[\"term\",\"tf_b2015\"])\n",
        "tf2016 = pd.read_csv(\"b2016_tf.csv\", sep=',', header= 0, names=[\"term\",\"tf_b2016\"])\n",
        "tf2017 = pd.read_csv(\"b2017_tf.csv\", sep=',', header= 0, names=[\"term\",\"tf_b2017\"])\n",
        "\n",
        "# 연도별 문서빈도 데이터 불러오기\n",
        "df1 = open(\"b2013.csv\",\"r\").readlines()\n",
        "df2 = open(\"b2014.csv\",\"r\").readlines()\n",
        "df3 = open(\"b2015.csv\",\"r\").readlines()\n",
        "df4 = open(\"b2016.csv\",\"r\").readlines()\n",
        "df5 = open(\"b2017.csv\",\"r\").readlines()\n",
        "\n",
        "# 연도별 문서 갯수 카운트\n",
        "DF2013 = len(df1)\n",
        "DF2014 = len(df2)\n",
        "DF2015 = len(df3)\n",
        "DF2016 = len(df4)\n",
        "DF2017 = len(df5)\n",
        "\n",
        "# 연도별 단어빈도 통합\n",
        "result = pd.merge(tf2013, tf2014)\n",
        "result = pd.merge(result, tf2015)\n",
        "result = pd.merge(result, tf2016)\n",
        "result = pd.merge(result, tf2017)\n",
        "\n",
        "# 2017년 단어빈도 기준 정렬\n",
        "result = result.sort_values([\"tf_b2017\"], ascending=[False])\n",
        "\n",
        "# 연도별 단어빈도 표준화 및 weight 적용\n",
        "result[\"TF2013\"] = (result[\"tf_b2013\"]/DF2013)*weight2013\n",
        "result[\"TF2014\"] = (result[\"tf_b2014\"]/DF2014)*weight2014\n",
        "result[\"TF2015\"] = (result[\"tf_b2015\"]/DF2015)*weight2015\n",
        "result[\"TF2016\"] = (result[\"tf_b2016\"]/DF2016)*weight2016\n",
        "result[\"TF2017\"] = (result[\"tf_b2017\"]/DF2017)*weight2017\n",
        "\n",
        "\n",
        "result1 = result[[\"tf_b2013\",\"tf_b2014\",\"tf_b2015\",\"tf_b2016\",\"tf_b2017\"]]\n",
        "\n",
        "result = result.drop([\"tf_b2013\",\"tf_b2014\",\"tf_b2015\",\"tf_b2016\",\"tf_b2017\"],1)\n",
        "\n",
        "\n",
        "result[\"AVERAGE\"] = result.mean(axis= 1)\n",
        "result[\"CAGR\"] = ((result[\"TF2017\"]/result[\"TF2013\"])**(1/4))-1\n",
        "result[\"TFA\"] = result1.mean(axis= 1)\n",
        "\n",
        "result = result.drop([\"TF2013\",\"TF2014\",\"TF2015\",\"TF2016\",\"TF2017\"],1)\n",
        "\n",
        "# 오름차\n",
        "result = result.sort_values([\"TFA\"], ascending=[False])\n",
        "result = result[:99]\n",
        "result.to_csv(\"test1.csv\", float_format='%.5f', index=False)\n",
        "\n",
        "# 내림차순\n",
        "#result[\"tf_\" + \"%s\" % sname] = result\n",
        "#result.to_csv(\"weaksignal.csv\", float_format='%.f', mode=\"a\", index=False)\n",
        "\n",
        "#    result = df1.merge(df2)\n",
        "#    result[\"TF-IDF\"] = result[\"tf\"]*numpy.log10(docu/result[\"df\"])\n",
        "\n",
        "#    df2 = pd.read_csv(\"%s\" % sname + \"_df.csv\", sep=',', header=0, names=[\"term\", \"df_\" + \"%s\" % sname])\n",
        "#    df2 = df2.sort_values([\"df_\" + \"%s\" % sname], ascending=[False])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "69k_fxnQ1iUV",
        "outputId": "952a20c2-a20e-4384-bdea-49b461bcd8f6"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>term</th>\n",
              "      <th>AVERAGE</th>\n",
              "      <th>CAGR</th>\n",
              "      <th>TFA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>정전</td>\n",
              "      <td>0.963213</td>\n",
              "      <td>0.078596</td>\n",
              "      <td>1012.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>발생</td>\n",
              "      <td>0.265647</td>\n",
              "      <td>0.193302</td>\n",
              "      <td>284.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>사고</td>\n",
              "      <td>0.214546</td>\n",
              "      <td>0.206506</td>\n",
              "      <td>234.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>가구</td>\n",
              "      <td>0.209200</td>\n",
              "      <td>0.067164</td>\n",
              "      <td>216.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>아파트</td>\n",
              "      <td>0.188616</td>\n",
              "      <td>0.106087</td>\n",
              "      <td>171.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420</th>\n",
              "      <td>천여</td>\n",
              "      <td>0.024904</td>\n",
              "      <td>-0.156244</td>\n",
              "      <td>25.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>가동</td>\n",
              "      <td>0.020635</td>\n",
              "      <td>-0.121309</td>\n",
              "      <td>25.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>분간</td>\n",
              "      <td>0.026411</td>\n",
              "      <td>0.024243</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>소방</td>\n",
              "      <td>0.027885</td>\n",
              "      <td>0.190131</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>현장</td>\n",
              "      <td>0.027305</td>\n",
              "      <td>0.061703</td>\n",
              "      <td>24.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    term   AVERAGE      CAGR     TFA\n",
              "8     정전  0.963213  0.078596  1012.8\n",
              "49    발생  0.265647  0.193302   284.8\n",
              "26    사고  0.214546  0.206506   234.0\n",
              "68    가구  0.209200  0.067164   216.8\n",
              "77   아파트  0.188616  0.106087   171.4\n",
              "..   ...       ...       ...     ...\n",
              "420   천여  0.024904 -0.156244    25.8\n",
              "199   가동  0.020635 -0.121309    25.2\n",
              "74    분간  0.026411  0.024243    25.0\n",
              "309   소방  0.027885  0.190131    25.0\n",
              "307   현장  0.027305  0.061703    24.8\n",
              "\n",
              "[99 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5jXHS7m1rD2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}