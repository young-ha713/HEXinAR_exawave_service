{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Package"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import csv\r\n",
    "from socket import MSG_MCAST\r\n",
    "from urllib.parse import quote, quote_plus\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd\r\n",
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 날짜 지정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strDate = '20200901' # 시작 날짜\r\n",
    "endDate = '20200930' # 종료 날짜"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CSV 저장"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = f\"daum_news_{strDate}-{endDate}.csv\"\r\n",
    "f = open(filename, \"w\", encoding=\"utf-8-sig\", newline=\"\")\r\n",
    "writer = csv.writer(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 유저 에이전트 지정\r\n",
    "> [WhatismyUserAgent(ctrl+🖱 후 확인)](https://www.whatismybrowser.com/detect/what-is-my-user-agent)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "headers = {\"User-Agent\" : \"개인 유저 에이전트\"}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 반복문에 들어갈 날짜 지정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dt_index = pd.date_range(start=strDate, end=endDate)\r\n",
    "dt_list = dt_index.strftime(\"%Y%m%d\").tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 스크래핑 시작"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in dt_list:\r\n",
    "    print('날짜',i)\r\n",
    "    page_url = f\"https://news.daum.net/breakingnews/?page=10000&regDate={i}\"\r\n",
    "    page =requests.get(page_url, headers=headers)\r\n",
    "    page_soup = BeautifulSoup(page.text, \"lxml\")\r\n",
    "    last_page = page_soup.find(\"em\", attrs=\"num_page\").get_text()\r\n",
    "    lastPage_num = re.sub(r'[^0-9]','',last_page)\r\n",
    "    # print(lastPage_num)\r\n",
    "\r\n",
    "    for j in range(1, int(lastPage_num)+1):\r\n",
    "        main_url = f\"https://news.daum.net/breakingnews/?page={j}&regDate={i}\" # url 입력\r\n",
    "        res = requests.get(main_url, headers=headers)\r\n",
    " \r\n",
    "        if res.status_code == 200:\r\n",
    "            print(i, int(lastPage_num), '중' ,j,'page',round(j/int(lastPage_num)*100, 2),'%', main_url, 'status:',res.status_code)\r\n",
    "            soup = BeautifulSoup(res.text, \"lxml\") # soup으로 저장\r\n",
    "            main = soup.find(\"ul\", attrs={\"class\":\"list_news2 list_allnews\"})\r\n",
    "            news = main.find_all(\"strong\", attrs={\"class\":\"tit_thumb\"})\r\n",
    "            cnt = 0\r\n",
    "            for new in news:\r\n",
    "                urls = new.select_one(\"a\")[\"href\"]# 페이지에 나와있는 뉴스 URL 변수 입력\r\n",
    "                # print(urls)\r\n",
    "                result = requests.get(urls, headers=headers)         # request 로 다시 개별 뉴스 접속\r\n",
    "                if result.status_code == 200:\r\n",
    "                    news_soup = BeautifulSoup(result.text, \"lxml\")\r\n",
    "                    # 뉴스 제목, 발행시간, 기사본문 저장\r\n",
    "                    title = news_soup.find(\"h3\", attrs={\"tit_view\"}).get_text().strip()\r\n",
    "                    pubdate = news_soup.find(\"span\", attrs={\"num_date\"}).get_text().strip()\r\n",
    "                    text = news_soup.find(\"div\", attrs={\"news_view\"}).get_text().strip()\r\n",
    "                    cnt += 1\r\n",
    "                    # print(j,'of',cnt,'번째 기사')\r\n",
    "                    # print(i,j,'of',cnt,'번째 기사', urls,'status:', result.status_code)\r\n",
    "                    writer.writerow([cnt, title, pubdate, urls, text])\r\n",
    "                else:\r\n",
    "                    print(i,j,'of',cnt,'번째 기사','error_code :',result.status_code, urls)\r\n",
    "                    pass\r\n",
    "                   \r\n",
    "        else:\r\n",
    "            print(i,'page : ',j,'error_code :',res.status_code, main_url)\r\n",
    "            pass"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.4 64-bit"
  },
  "interpreter": {
   "hash": "4c65b98e956c6ae24f8ae0bc56d1e465ff92310dbdec0a4bd6b48ffdf1441c98"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}