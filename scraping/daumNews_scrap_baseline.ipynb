{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Package"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import csv\r\n",
    "from socket import MSG_MCAST\r\n",
    "from urllib.parse import quote, quote_plus\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 날짜 지정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strDate = '20200901' # 시작 날짜\r\n",
    "endDate = '20200930' # 종료 날짜"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CSV 저장"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = f\"daum_news_{strDate}-{endDate}.csv\"\r\n",
    "f = open(filename, \"w\", encoding=\"utf-8-sig\", newline=\"\")\r\n",
    "writer = csv.writer(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 반복문에 들어갈 날짜 지정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dt_index = pd.date_range(start=strDate, end=endDate)\r\n",
    "dt_list = dt_index.strftime(\"%Y%m%d\").tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 스크래핑 시작"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cnt = 0\r\n",
    "max_page = 10000 # 뉴스 페이지 탭 수 지정\r\n",
    "\r\n",
    "for i in dt_list:\r\n",
    "    print(i)\r\n",
    "    try:\r\n",
    "        for j in range(1, max_page):\r\n",
    "            # print(j, j+1)\r\n",
    "            main_url = f\"https://news.daum.net/breakingnews/?page={j}&regDate={i}\" # url 입력\r\n",
    "            res = requests.get(main_url)\r\n",
    "            res.raise_for_status()\r\n",
    "            soup = BeautifulSoup(res.text, \"lxml\") # soup으로 저장\r\n",
    "            main = soup.find(\"ul\", attrs={\"class\":\"list_news2 list_allnews\"})\r\n",
    "\r\n",
    "            val_url = f\"https://news.daum.net/breakingnews/?page={j+1}&regDate={i}\"\r\n",
    "            val = requests.get(val_url)\r\n",
    "            val.raise_for_status()\r\n",
    "            val_soup = BeautifulSoup(val.text, \"lxml\")\r\n",
    "            val_main = val_soup.find(\"ul\", attrs={\"class\":\"list_news2 list_allnews\"})\r\n",
    "            \r\n",
    "            if main != val_main:\r\n",
    "                try:\r\n",
    "                    news = main.find_all(\"strong\", attrs={\"class\":\"tit_thumb\"})\r\n",
    "                    for new in news:\r\n",
    "                        urls = new.select_one(\"a\")[\"href\"]# 페이지에 나와있는 뉴스 URL 변수 입력\r\n",
    "                        # print(urls)\r\n",
    "\r\n",
    "                        result = requests.get(urls)         # request 로 다시 개별 뉴스 접속\r\n",
    "                        result.raise_for_status()\r\n",
    "                        news_soup = BeautifulSoup(result.text, \"lxml\")\r\n",
    "                        # 뉴스 제목, 발행시간, 기사본문 저장\r\n",
    "                        title = news_soup.find(\"h3\", attrs={\"tit_view\"}).get_text().strip()\r\n",
    "                        pubdate = news_soup.find(\"span\", attrs={\"num_date\"}).get_text().strip()\r\n",
    "                        text = news_soup.find(\"div\", attrs={\"news_view\"}).get_text().strip()\r\n",
    "                        cnt += 1\r\n",
    "                        # print(cnt, pubdate)\r\n",
    "                        writer.writerow([cnt, title, pubdate, urls, text])\r\n",
    "                except Exception as e:\r\n",
    "                    print(\"오류내용 :\",e)\r\n",
    "                    pass\r\n",
    "            else:\r\n",
    "                try:\r\n",
    "\r\n",
    "                    news = main.find_all(\"strong\", attrs={\"class\":\"tit_thumb\"})\r\n",
    "                    for new in news:\r\n",
    "                        urls = new.select_one(\"a\")[\"href\"]# 페이지에 나와있는 뉴스 URL 변수 입력\r\n",
    "                        # print(urls)\r\n",
    "\r\n",
    "                        result = requests.get(urls)         # request 로 다시 개별 뉴스 접속\r\n",
    "                        result.raise_for_status()\r\n",
    "                        news_soup = BeautifulSoup(result.text, \"lxml\")\r\n",
    "                        # 뉴스 제목, 발행시간, 기사본문 저장\r\n",
    "                        title = news_soup.find(\"h3\", attrs={\"tit_view\"}).get_text().strip()\r\n",
    "                        pubdate = news_soup.find(\"span\", attrs={\"num_date\"}).get_text().strip()\r\n",
    "                        text = news_soup.find(\"div\", attrs={\"news_view\"}).get_text().strip()\r\n",
    "                        cnt += 1\r\n",
    "                        # print(cnt, pubdate)\r\n",
    "                        writer.writerow([cnt, title, pubdate, urls, text])\r\n",
    "                    break\r\n",
    "                except Exception as e:\r\n",
    "                    print(\"오류내용 :\",e)\r\n",
    "                    pass\r\n",
    "    except Exception as e:\r\n",
    "        print(\"오류내용 :\",e)\r\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.4 64-bit"
  },
  "interpreter": {
   "hash": "68435f9a382c9677d2dcae99d2d139da3415a3d063b38295be49e8c3f2ac11d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}